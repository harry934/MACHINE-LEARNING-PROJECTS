{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harry934/MACHINE-LEARNING-PROJECTS/blob/main/Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling on HateSpeech-Kenya Dataset\n",
        "Dataset: https://www.kaggle.com/datasets/edwardombui/hatespeech-kenya\n",
        "\n",
        "### Workflow\n",
        "<ol><li>Load the dataset (hatespeech-kenya).\n",
        "<li>Preprocess the data (cleaning, tokenization, etc.).\n",
        "<li> Build a dictionary and document-term matrix (use 5000 features)\n",
        "<li> Train an LDA model using gensim.\n",
        "<li> Visualize topics using pyLDAvis.\n",
        "<li> Evaluate topics for interpretability.</ol>"
      ],
      "metadata": {
        "id": "wjRgd6yNKjaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Install required packages (run once)\n",
        "!pip install gensim pyLDAvis pandas numpy matplotlib seaborn scikit-learn wordcloud nltk -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsKpz3C7LF41",
        "outputId": "6d682765-9787-49c5-9a98-5752a874363a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xaLOh-wKd0B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(\"Libraries imported and NLTK data downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load the dataset"
      ],
      "metadata": {
        "id": "Qu4X7RTuLPLa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c90930e"
      },
      "source": [
        "After running the above cell, you should be able to execute the Kaggle download command without the `OSError`. You can now re-run the cell where you attempted to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the CSV file from Kaggle: \"HateSpeech_Kenya.csv\"\n",
        "\n",
        "df = pd.read_csv('/content/HateSpeech_Kenya.csv')\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Dc04orEPLHrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore class distribution (if labeled)\n",
        "print(df['Class'].value_counts())  # or adjust column name"
      ],
      "metadata": {
        "id": "ZSIny2FYLb-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Text Preprocessing"
      ],
      "metadata": {
        "id": "u2-U09vsLxKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the correct text column (common names)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text_col = None\n",
        "for col in ['Tweet']:\n",
        "    if col in df.columns:\n",
        "        text_col = col\n",
        "        break\n",
        "\n",
        "if text_col is None:\n",
        "    # If not found, print columns and pick manually\n",
        "    print(\"Available columns:\", df.columns.tolist())\n",
        "    text_col = input(\"Enter the name of the text column: \")\n",
        "\n",
        "print(f\"Using text column: {text_col}\")\n",
        "\n",
        "# Extract texts\n",
        "texts = df[text_col].astype(str)\n",
        "\n",
        "# Preprocessing function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Add common Swahili/ Sheng / Kenyan online slang stopwords (optional but helpful)\n",
        "extra_stopwords = {'ni', 'wa', 'na', 'ya', 'kwa', 'ni', 'lakini', 'nawe', 'mimi', 'wewe', 'yeye',\n",
        "                   'sisi', 'nyinyi', 'hao', 'hii', 'hizi', 'hiyo', 'hizo', 'hapa', 'pale', 'humu',\n",
        "                   'kule', 'ndio', 'hapana', 'bila', 'kila', 'baada', 'kabla', 'hadi', 'zaidi',\n",
        "                   'si', 'je', 'kwani', 'kwamba', 'ili', 'ambao', 'ambayo', 'ambazo', 'watu',\n",
        "                   'msee', 'bro', 'sis', 'dem', 'aki', 'ama', 'sasa', 'tu', 'nioe', 'wasee'}\n",
        "stop_words.update(extra_stopwords)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
        "    # Remove mentions (@user) and hashtags\n",
        "    text = re.sub(r'@\\w+|#\\w+', ' ', text)\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and short words\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    # Optional: Stemming (can also use lemmatization)\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "print(\"Preprocessing texts...\")\n",
        "processed_texts = texts.apply(preprocess_text)\n",
        "\n",
        "# Remove empty documents\n",
        "processed_texts = processed_texts[processed_texts.apply(len) > 0]\n",
        "\n",
        "print(f\"Number of documents after cleaning: {len(processed_texts)}\")\n",
        "print(\"Sample processed text:\", processed_texts.iloc[0])"
      ],
      "metadata": {
        "id": "uSu-P_-hLqpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create Dictionary and Document-Term Matrix (with max 5000 features)"
      ],
      "metadata": {
        "id": "KEjya7x3L025"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(processed_texts)\n",
        "\n",
        "# Filter extremes: remove terms appearing in <5 docs or >70% of docs\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.7, keep_n=5000)\n",
        "\n",
        "print(f\"Dictionary size after filtering: {len(dictionary)} tokens\")\n",
        "\n",
        "# Create Bag-of-Words corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "print(f\"Corpus created with {len(corpus)} documents.\")"
      ],
      "metadata": {
        "id": "crIfdHc0L20T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Train LDA Model using Gensim"
      ],
      "metadata": {
        "id": "6p5prEmkMMMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal number of topics? Let's try 8–12 for hate speech data (ethnicity, politics, gender, etc.)\n",
        "NUM_TOPICS = 10\n",
        "\n",
        "lda_model = LdaMulticore(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=NUM_TOPICS,\n",
        "    random_state=42,\n",
        "    chunksize=2000,\n",
        "    passes=10,\n",
        "    alpha='symmetric',\n",
        "    eta='auto',\n",
        "    workers=4,  # adjust based on your CPU\n",
        "    eval_every=1,\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "print(\"LDA Model Training Completed!\")"
      ],
      "metadata": {
        "id": "gozgX6JdMIQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display top words per topic\n",
        "print(\"Top words per topic:\\n\")\n",
        "for idx in range(NUM_TOPICS):\n",
        "    print(f\"Topic #{idx + 1}:\")\n",
        "    words = lda_model.print_topic(idx, topn=10)\n",
        "    print(words)\n",
        "    print()"
      ],
      "metadata": {
        "id": "iqI02VOVMQ-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Visualize with pyLDAvis"
      ],
      "metadata": {
        "id": "GLnauyo8MYHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare visualization\n",
        "vis = gensimvis.prepare(lda_model, corpus, dictionary, mds='mmds', sort_topics=False)\n",
        "\n",
        "# Display in notebook\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "CwsfZW9HMVcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Evaluate Topics for Interpretability\n"
      ],
      "metadata": {
        "id": "Gn_qoqMfMjoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Coherence Score (higher is better)\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"C_V Coherence Score: {coherence_score:.4f}\")\n",
        "\n",
        "# Manual interpretation (example labels based on common Kenyan hate speech themes)\n",
        "topic_labels = {\n",
        "    0: \"Ethnic Attacks (Luo vs Kikuyu)\",\n",
        "    1: \"Political Incitement & Election Violence\",\n",
        "    2: \"Gender-Based Hate & Misogyny\",\n",
        "    3: \"Religious Intolerance\",\n",
        "    4: \"Raila/Ruto Political Tribalism\",\n",
        "    5: \"General Insults & Profanity\",\n",
        "    6: \"Anti-Kikuyu Sentiment\",\n",
        "    7: \"Calls for Violence/Genocide Rhetoric\",\n",
        "    8: \"Anti-Kalenjin Hate\",\n",
        "    9: \"Police Brutality & State Criticism\"\n",
        "}\n",
        "\n",
        "print(\"\\nSuggested Topic Labels (based on inspection):\")\n",
        "for i, label in topic_labels.items():\n",
        "    # Corrected line: Extract the word (first element) from each tuple\n",
        "    top_words = [word_prob[0] for word_prob in lda_model.show_topic(i, 10)]\n",
        "    print(f\"Topic {i+1}: {label}\")\n",
        "    print(\"   →\", \", \".join(top_words))\n",
        "    print()"
      ],
      "metadata": {
        "id": "vw7AhIzmMgnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find dominant topic for each document\n",
        "def get_dominant_topic(ldamodel, corpus):\n",
        "    topics = []\n",
        "    for doc in corpus:\n",
        "        topic_probs = ldamodel.get_document_topics(doc)\n",
        "        dominant = max(topic_probs, key=lambda x: x[1])\n",
        "        topics.append((dominant[0], dominant[1]))\n",
        "    return topics\n",
        "\n",
        "df_clean = df.loc[processed_texts.index].copy()\n",
        "dominant_topics = get_dominant_topic(lda_model, corpus)\n",
        "df_clean['dominant_topic'] = [t[0] + 1 for t in dominant_topics]\n",
        "df_clean['topic_prob'] = [t[1] for t in dominant_topics]\n",
        "\n",
        "print(\"Dominant topic distribution:\")\n",
        "print(df_clean['dominant_topic'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "KplsfH2MMuxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Steps You Can Add:\n",
        "\n",
        "Hyperparameter tuning (grid search over number of topics using coherence)<br>\n",
        "Compare with BERTopic (modern alternative)\n",
        "Classify hate vs non-hate using topic proportions as features"
      ],
      "metadata": {
        "id": "vEX2vtxkM4Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7yIe9wOQM5lW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}